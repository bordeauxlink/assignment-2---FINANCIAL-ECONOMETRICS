{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "file_path = '/data.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "def lag_and_square(ser, n):\n",
    "    # Create a DataFrame with the original series as the first column\n",
    "    df = pd.DataFrame({'Return': ser})\n",
    "\n",
    "    # Generate lagged and squared lagged columns\n",
    "    for i in range(1, n+1):\n",
    "        lag_col = f'Lag_{i}'\n",
    "        squared_lag_col = f'Squared_Lag_{i}'\n",
    "\n",
    "        # Create lagged column\n",
    "        df[lag_col] = ser.shift(i)\n",
    "\n",
    "        # Create squared lagged column\n",
    "        df[squared_lag_col] = df[lag_col] ** 2\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    return df.dropna()\n",
    "\n",
    "# Example usage:\n",
    "lagged_df = lag_and_square(df['RET'], 2)\n",
    "print(lagged_df.head(5))\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Function for OLS regression\n",
    "def OLS_regression(df, n):\n",
    "    # Use your existing function to create a DataFrame with lagged and squared lagged values\n",
    "    lagged_df = lag_and_square(df.iloc[:,0], n)  # assuming df has only one column\n",
    "\n",
    "    # Prepare X and y for OLS regression\n",
    "    X = lagged_df.iloc[:, 1:]  # Exclude the 'Return' column which is the dependent variable\n",
    "    X = sm.add_constant(X)     # Add a constant term for the intercept\n",
    "    y = lagged_df['Return']\n",
    "\n",
    "    # Perform OLS regression\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "n = 2\n",
    "result_model = OLS_regression(lagged_df, n)\n",
    "print(result_model.summary())\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def plot_ridge_coeffs(df, s, n):\n",
    "    # Assuming the first column is the dependent variable\n",
    "    y = df.iloc[:, 0]\n",
    "    # All remaining columns are used as independent variables in the regression\n",
    "    X = df.iloc[:, 1:]\n",
    "\n",
    "    # Add a constant term to the independent variables\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Generate n values for the regularization parameter λ, evenly spaced between 0 and s\n",
    "    alphas = np.linspace(0, s, n)\n",
    "\n",
    "    # Initialize a list to store the coefficients for each λ\n",
    "    coefs = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        # Create and fit the Ridge regression model for each value of alpha\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        ridge.fit(X, y)\n",
    "        # Append the coefficients from the regression to the coefs list\n",
    "        coefs.append(ridge.coef_)\n",
    "\n",
    "    # Plot the Ridge coefficients against λ\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(1, X.shape[1]):  # Start at 1 to skip the constant term\n",
    "        plt.plot(alphas, [coef[i] for coef in coefs], label=f'Coef for {X.columns[i]}')\n",
    "\n",
    "    plt.xlabel('Regularization Parameter λ')\n",
    "    plt.ylabel('Coefficient Value')\n",
    "    plt.title('Ridge Regression Coefficients vs. Regularization Parameter')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "n_lags = 2  #Use this to adjust the number of lagged variables in the plot\n",
    "df_lagged = lag_and_square(df['RET'], n_lags)  # This will create additional lagged variables based on n_lags\n",
    "\n",
    "\n",
    "# Using the function with the df_lagged DataFrame\n",
    "# Set the value of s and n\n",
    "s_value = 1  # This should be a positive real scalar\n",
    "n_value = 100  # This should be a positive integer\n",
    "\n",
    "# Call the function with the lagged DataFrame\n",
    "plot_ridge_coeffs(df_lagged, s_value, n_value)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def ridge_regression_cv(df, s, n, LOOCV):\n",
    "    # Extract the dependent variable (first column) and independent variables\n",
    "    y = df.iloc[:, 0]\n",
    "    X = df.iloc[:, 1:]\n",
    "\n",
    "    # Standardize X\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Add constant to the scaled data for the regression intercept\n",
    "    X_scaled_with_constant = sm.add_constant(X_scaled)\n",
    "\n",
    "    # Define the range of lambda values to test\n",
    "    alphas = np.linspace(1e-8, s, n)\n",
    "\n",
    "    # Perform Ridge Regression with Cross-Validation\n",
    "    if LOOCV:\n",
    "        ridge_cv = RidgeCV(alphas=alphas, cv=None)  # LOOCV\n",
    "    else:\n",
    "        ridge_cv = RidgeCV(alphas=alphas, cv=10)  # 10-fold CV\n",
    "\n",
    "    ridge_cv.fit(X_scaled_with_constant, y)\n",
    "\n",
    "    # Extract the optimal shrinkage parameter and coefficients\n",
    "    best_lambda = ridge_cv.alpha_\n",
    "    best_coefs = ridge_cv.coef_\n",
    "    return best_lambda, best_coefs\n",
    "\n",
    "# Example usage\n",
    "n_lags = 2 #Use this to adjust the number of lagged variables\n",
    "df_lagged1 = lag_and_square(df['RET'], n_lags)  # This will create additional lagged variables based on n_lags\n",
    "\n",
    "best_lambda, best_coefs = ridge_regression_cv(df_lagged1, 10, 100, LOOCV=True)\n",
    "print(\"Optimal shrinkage parameter (lambda):\", best_lambda)\n",
    "print(\"Coefficients at optimal shrinkage parameter:\", best_coefs)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def ridge_regression_cv(df, s, n, LOOCV):\n",
    "    # Extract the dependent variable (first column) and independent variables\n",
    "    y = df.iloc[:, 0]\n",
    "    X = df.iloc[:, 1:]\n",
    "\n",
    "    # Standardize X\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_with_constant = sm.add_constant(X_scaled)  # Add constant after scaling\n",
    "\n",
    "    # Define the range of lambda values to test on a logarithmic scale\n",
    "    alphas = np.logspace(-8, np.log10(s), n)\n",
    "\n",
    "    # Perform Ridge Regression with Cross-Validation\n",
    "    if LOOCV:\n",
    "        ridge_cv = RidgeCV(alphas=alphas, cv=len(y))  # LOOCV\n",
    "    else:\n",
    "        ridge_cv = RidgeCV(alphas=alphas, cv=10)  # 10-fold CV\n",
    "\n",
    "    ridge_cv.fit(X_scaled_with_constant, y)\n",
    "\n",
    "    # Extract the optimal shrinkage parameter and coefficients\n",
    "    best_lambda = ridge_cv.alpha_\n",
    "    best_coefs = ridge_cv.coef_\n",
    "\n",
    "    return best_lambda, best_coefs\n",
    "\n",
    "# Example usage\n",
    "n_lags = 2 #Use this to adjust the number of lagged variables\n",
    "df_lagged1 = lag_and_square(df['RET'], n_lags)  # This will create additional lagged variables based on n_lags\n",
    "\n",
    "best_lambda, best_coefs = ridge_regression_cv(df_lagged1, 1, 2, LOOCV=False)\n",
    "print(\"Optimal shrinkage parameter (lambda):\", best_lambda)\n",
    "print(\"Coefficients at optimal shrinkage parameter:\", best_coefs)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def plot_ridge_coeffs(df, s, n):\n",
    "    # Assuming the first column is the dependent variable\n",
    "    y = df.iloc[:, 0]\n",
    "    # All other columns are independent variables\n",
    "    X = df.iloc[:, 1:]\n",
    "\n",
    "    # Standardize X and add constant\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = sm.add_constant(X_scaled)\n",
    "\n",
    "    # Generate n values for the regularization parameter λ\n",
    "    alphas = np.linspace(0, s, n)\n",
    "\n",
    "    # Initialize a list to store the coefficients\n",
    "    coefs = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        # Fit Ridge regression with current alpha\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        ridge.fit(X_scaled, y)\n",
    "        # Store coefficients\n",
    "        coefs.append(ridge.coef_)\n",
    "\n",
    "    # Plotting the coefficients\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(X_scaled.shape[1] - 1):  # Exclude the constant\n",
    "        plt.plot(alphas, [coef[i+1] for coef in coefs], label=f'Coef {i+1}')  # i+1 to skip constant\n",
    "\n",
    "    plt.xlabel('Regularization Parameter (λ)')\n",
    "    plt.ylabel('Coefficient Value')\n",
    "    plt.title('Ridge Regression Coefficients vs. Regularization Parameter')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "# Example usage with the lagged_df DataFrame\n",
    "plot_ridge_coeffs(lagged_df, s=1, n=10)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def plot_ridge_coefficients(df, s, n):\n",
    "    # Define the dependent variable (y) and independent variables (X)\n",
    "    y = df.iloc[:, 0]\n",
    "    X = df.iloc[:, 1:]\n",
    "    X = sm.add_constant(X)  # Add a constant to the model\n",
    "\n",
    "    # Create n values of the regularization parameter λ evenly spaced between 0 and s\n",
    "    alphas = np.linspace(0, s, n)\n",
    "\n",
    "    # Initialize a list to store the coefficients for each Ridge regression\n",
    "    coefficients = []\n",
    "\n",
    "    # Run Ridge regression for each value of λ and store the coefficients\n",
    "    for alpha in alphas:\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        ridge.fit(X, y)\n",
    "        coefficients.append(ridge.coef_)\n",
    "\n",
    "    # Convert the list of coefficients to a DataFrame for easier plotting\n",
    "    coefficients_df = pd.DataFrame(coefficients, index=alphas, columns=X.columns)\n",
    "\n",
    "    # Plot the coefficients\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for column in coefficients_df.columns:\n",
    "        plt.plot(coefficients_df.index, coefficients_df[column], label=column)\n",
    "    plt.xlabel('Regularization Parameter λ')\n",
    "    plt.ylabel('Coefficients')\n",
    "    plt.title('Ridge Regression Coefficients vs. Shrinkage')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(X,y)\n",
    "\n",
    "# Example usage with the lagged DataFrame\n",
    "plot_ridge_coefficients(lagged_df, s=1, n=10)\n",
    "\n",
    "\n",
    "\n",
    "def ridge_plot(df, s, n):\n",
    "    df_lagged = lag_and_square(df, n)\n",
    "    X = df_lagged.drop(columns=['Return'])\n",
    "    X = sm.add_constant(X)\n",
    "    y = df_lagged['Return']\n",
    "    # Standardize the independent variables\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    alphas = np.linspace(0, s, n)\n",
    "    coefs = []\n",
    "    for alpha in alphas:\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        ridge.fit(X, y)\n",
    "        coefs.append(ridge.coef_)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, col_name in enumerate(df_lagged.drop(columns=['Return']).columns):\n",
    "        plt.plot(alphas, [coef[i] for coef in coefs], label=col_name)\n",
    "    plt.xlabel('Shrinkage Parameter (λ)')\n",
    "    plt.ylabel('Coefficients')\n",
    "    plt.title('Ridge Regression Coefficients vs. Shrinkage Parameter')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "ridge_plot(df['RET'], 1, 10)\n",
    "\n",
    "def ridge_plot(df, s, n):\n",
    "    df_lagged = lag_and_square(df, n)\n",
    "    X = df_lagged.drop(columns=['Return'])\n",
    "    X = sm.add_constant(X)\n",
    "    y = df_lagged['Return']\n",
    "    # Standardize the independent variables\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    alphas = np.linspace(0, s, n)\n",
    "    coefs = []\n",
    "    for alpha in alphas:\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        ridge.fit(X_scaled, y)\n",
    "        coefs.append(ridge.coef_)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Exclude the coefficient for the constant term when plotting\n",
    "    for i, col_name in enumerate(df_lagged.drop(columns=['Return']).columns):\n",
    "        plt.plot(alphas, [coef[i + 1] for coef in coefs], label=col_name)\n",
    "    plt.xlabel('Shrinkage Parameter (λ)')\n",
    "    plt.ylabel('Coefficients')\n",
    "    plt.title('Ridge Regression Coefficients vs. Shrinkage Parameter')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(X,y)\n",
    "\n",
    "ridge_plot(df['RET'], 1, 5)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
